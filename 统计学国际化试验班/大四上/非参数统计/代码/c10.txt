##Chapter 10: nonparametric regression (curve smoothing)

### Histogram (no discussions on bin size)
set.seed(179011)
x = rnorm(100,0,1);  x = sort(x);  f0 = dnorm(x)

par(mfrow=c(1,2))
hist(x)
hist(x, prob=1); lines(f0~x,col=2)
h=1.75*sd(x)/(length(x))^(1/3)
hist(x, prob=1, breaks=seq(min(x)-h, max(x)+h,h), main=paste("h=",2*h));  lines(f0~x,col=2)
hist(x, prob=1, breaks=round((max(x)-min(x))/h));  lines(f0~x,col=2)


###Histogram vs KDE 
duration = faithful$eruptions;  n=length(duration)
par(mfrow=c(1,2))
h=1.75*sd(duration)/length(duration)^(1/3)
out=hist(duration, breaks=round((max(x)-min(x))/h), prob=1)
	
###kernel density estimation
d1 = density(duration, kernel="gaussian"); lines(d1)
d2=density(duration, kernel="triangular"); lines(d2, col="red")

# use CV to select optimal bandwidth, might tend to undersmooth
library(kedd)
h0=h.mlcv(duration, deriv.order=0)$h
hatf=dkde(duration, deriv.order=0, h=h0)
hist(duration, prob=1, ylim=c(0, max(c(hatf$est.fx, out$density))))
#lines(hatf$eval.points, hatf$est.fx, type="l")
lines(density(duration, kernel="gaussian", bw=h0), col=2)


######## Section 2: nonparametric curve smoothing
### Example: motor cycle
library(MASS)
data(mcycle);  x = mcycle[,1];  y = mcycle[,2];  n=length(x)

par(mfrow=c(2,2))
### polynomial fit
plot(y~x, main="scatter plot")
x2 = x^2; x3=x^3; fit2 = lm(y~x+x2+x3)$fitted;  lines(fit2~x, col=2)

### loess 
lo <- loess(y ~ x) 
newx = seq(min(x), max(x), length=50)
pred =predict(lo, data.frame(x = newx))
lines(pred ~newx, col=2)
# span selection  (no simple expression for CV)
loa=function(s1){
s= loess(y~x,span=s1, degree=0)$s;  return(s) }         # equivalent to GCV criterion 
lob=function(s1){
lo= loess(y~x,span=s1, degree=0)
res=resid(lo);  AIC=log(sum(res^2))+2*lo$enp/lo$n;  return(AIC) }

ss=optimize(loa, c(0.1, 0.8))$min
fss=loess(y~x, span=ss)
lines(fitted(fss)~x, col=2)

ss2=optimize(lob, c(0.1, 0.8))$min
fss2=loess(y~x, span=ss2)
lines(sort(x), predict(fss2, newdata=data.frame(x=sort(x))), col=3) 

### kernel regression 
library(locfit)
gcv.b<-gcvplot(y~x, kern="gauss", deg=0, ev=dat(), alpha=seq(0.01, 0.99, by=.01))  # use GCV
alpha.chosen.b = gcv.b$alpha[which.min(gcv.b$values)]
fit.b <- locfit(y~x, kern="gauss",deg=0,  ev=dat(), alpha=alpha.chosen.b )
plot(x, y, xlab="X", ylab="Y", col="darkgrey")
lines(x, fitted(fit.b), col="green", lwd=2) 

AIC=log((n-gcv.b$df)*gcv.b$values)+2*gcv.b$df/n  # calculate AIC
alpha.chosen.c=gcv.b$alpha[which.min(AIC)]
fit.c <- locfit(y~x, kern="gauss",deg=0,  ev=dat(), alpha=alpha.chosen.c )
lines(x, fitted(fit.c), col="green", lwd=2) 

### local polynomial

gcv.b1 <- gcvplot(y~x, kern="gauss", deg=1, ev=dat(), alpha=seq(0.01, 0.99, by=.01))
alpha.chosen.b1 = gcv.b1$alpha[which.min(gcv.b1$values)]     
fit.b1 <- locfit(y~x, kern="gauss",deg=1, ev=dat(), alpha= alpha.chosen.b1) 
lines(x, fitted(fit.b1), col="green", lwd=2)
#Note: you could extract the df from gcv.b1$df or fit.b1$dp["df1"], and calculate AIC. 


### spline methods
library(mgcv)

fit.d=gam(y~s(x))

z=seq(min(x), max(x), by=0.5)
pred.d=predict(fit.d, newdata=data.frame(x=z), se.fit=TRUE)

plot(x,y)
lines(z, pred.d$fit)
lines(z, pred.d$fit-1.96*pred.d$se.fit)
lines(z, pred.d$fit+1.96*pred.d$se.fit)


# Discussions on the choice of smothing parameters 
summary(fit.d)
fit.d$sp  # Extract smoothing paramter
fit.d$gcv.ubre  # GCV
fit.d$aic       # AIC 


# Two ways to extract diagonal elements of the "hat" matrix
fit.d$hat  
influence(fit.d) 


sp =10^(seq(-5, -3, by=0.02)) # this seems to be a reasonable range given fit.c$sp is equal to 0.32
m = length(sp)
GCV = DF = AIC= numeric(m)
n = length(x)[1]	
for (i in 1:m) {
   fit = gam( y ~ s(x), sp=sp[i] )
   DF[i] = sum( influence(fit) )  # trace of hat matrix
   GCV[i] =fit$gcv.ubre 
   AIC[i] =fit$aic
}


fit.d1=gam(y~s(x), sp=sp[which.min(GCV)])
fit.d2=gam(y~s(x), sp=sp[which.min(AIC)])

plot(x,y)
lines(z, predict(fit.d1, data.frame(x=z)), col="red")
lines(z, predict(fit.d2, data.frame(x=z)), col="blue")


#

library(AER)
library(mgcv)
data(CPS1988)
attach(CPS1988)
fitGam = gam(log(wage)~s(education)+s(experience)+ethnicity)


summary(fitGam)
par(mfrow=c(1,2))
plot(fitGam)

